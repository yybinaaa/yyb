import json
import pickle
import struct
from pathlib import Path

import numpy as np
import torch
from tqdm import tqdm


# def save_emb(emb, save_path):
#     """
#     å°†Embeddingä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
#     """
#     num_points, num_dimensions = emb.shape
#     print(f'æ­£åœ¨ä¿å­˜ {save_path}')
#     with open(Path(save_path), 'wb') as f:
#         f.write(struct.pack('II', num_points, num_dimensions))
#         emb.tofile(f)


# def load_mm_emb(mm_path, feat_ids):
#     """
#     åŠ è½½å¤šæ¨¡æ€ç‰¹å¾Embedding
#     """
#     SHAPE_DICT = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
#     mm_emb_dict = {}
#     for feat_id in tqdm(feat_ids, desc='åŠ è½½å¤šæ¨¡æ€ç‰¹å¾'):
#         shape = SHAPE_DICT[feat_id]
#         emb_dict = {}
#         if feat_id == '81':
#             with open(Path(mm_path, f'emb_{feat_id}_{shape}.pkl'), 'rb') as f:
#                 emb_dict = pickle.load(f)
#         else:
#             try:
#                 base_path = Path(mm_path, f'emb_{feat_id}_{shape}')
#                 for json_file in base_path.glob('*.json'):
#                     with open(json_file, 'r', encoding='utf-8') as file:
#                         for line in file:
#                             data = json.loads(line.strip())
#                             emb = np.array(data['emb'], dtype=np.float32) if isinstance(data['emb'], list) else data['emb']
#                             emb_dict[data['anonymous_cid']] = emb
#             except Exception as e:
#                 print(f"è½¬æ¢é”™è¯¯: {e}")
#         mm_emb_dict[feat_id] = emb_dict
#         print(f'å·²åŠ è½½ #{feat_id} å¤šæ¨¡æ€ç‰¹å¾')
#     return mm_emb_dict




# class MyDataset(torch.utils.data.Dataset):
#     # æ–°çš„ __init__ æ–¹æ³•
#     def __init__(self, data_dir, args):
#         super().__init__()
#         self.data_dir = Path(data_dir)
#         self.maxlen = args.maxlen
#         self.mm_emb_ids = args.mm_emb_id

#         # <<< æ ¸å¿ƒä¿®æ”¹ 1: åŠ è½½ preprocess.py ç”Ÿæˆçš„ä¸‰ä¸ªæ ¸å¿ƒæ–‡ä»¶ >>>
#         print("ğŸ’¾ æ­£åœ¨åŠ è½½é¢„å¤„ç†æ–‡ä»¶ (indexer, user_features, item_features)...")
#         with open(self.data_dir / 'indexer.pkl', 'rb') as f:
#             self.indexer = pickle.load(f)
#         with open(self.data_dir / 'user_features.pkl', 'rb') as f:
#             self.user_features = pickle.load(f)
#         with open(self.data_dir / 'item_features.pkl', 'rb') as f:
#             self.item_features = pickle.load(f)
#         print("âœ… é¢„å¤„ç†æ–‡ä»¶åŠ è½½å®Œæ¯•!")

#         # <<< æ ¸å¿ƒä¿®æ”¹ 2: ç›´æ¥ä»æ–°çš„ indexer è·å–æ‰€æœ‰å…ƒä¿¡æ¯ >>>
#         self.usernum = len(self.indexer['u'])
#         self.itemnum = len(self.indexer['i'])
#         # åˆ›å»ºä¸€ä¸ªä» ç‰©å“åŸå§‹ID -> ç‰©å“æ•´æ•°ç´¢å¼• çš„åå‘æ˜ å°„ï¼Œæ–¹ä¾¿åé¢æŸ¥æ‰¾
#         self.item_original_id_to_idx = self.indexer['i']

#         # <<< æ ¸å¿ƒä¿®æ”¹ 3: ç¡¬ç¼–ç ç‰¹å¾é”®ï¼Œå¹¶ä» indexer è·å–è¯å…¸å¤§å° >>>
#         # è¿™é‡Œçš„é¡ºåºå¿…é¡»å’Œ preprocess.py ä¸­å®šä¹‰çš„å®Œå…¨ä¸€è‡´
#         self.USER_SPARSE_FEAT_KEYS = ['103', '104', '105', '109']
#         self.ITEM_SPARSE_FEAT_KEYS = ['100', '117', '111', '118', '101', '102', '119', '120', '114', '112', '121', '115', '122', '116']
        
#         # è¿™ä¸ªä¿¡æ¯å°†ä¼ é€’ç»™æ¨¡å‹ï¼Œå‘Šè¯‰æ¨¡å‹æ¯ä¸ªç‰¹å¾çš„ Embedding å±‚åº”è¯¥å»ºå¤šå¤§
#         self.feat_statistics = {
#             feat_id: len(feat_map) for feat_id, feat_map in self.indexer['f'].items()
#         }
#         self.feature_types = {
#             'user_sparse': self.USER_SPARSE_FEAT_KEYS,
#             'item_sparse': self.ITEM_SPARSE_FEAT_KEYS,
#             'item_emb': self.mm_emb_ids,
#         }

#         # <<< æ ¸å¿ƒä¿®æ”¹ 4: å¤šæ¨¡æ€ç‰¹å¾éƒ¨åˆ†ä¿æŒä¸å˜ >>>
#         EMB_SHAPE_DICT = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
#         self.ITEM_EMB_FEAT = {k: EMB_SHAPE_DICT[k] for k in self.feature_types['item_emb']}

#         # è¿™ä¸ªæ–¹æ³•åŠ è½½åŸå§‹åºåˆ—æ–‡ä»¶ï¼Œä¿æŒä¸å˜
#         self._load_data_and_offsets()

#     def _load_data_and_offsets(self):
#         self.data_file = open(self.data_dir / "seq.jsonl", 'rb')
#         with open(Path(self.data_dir, 'seq_offsets.pkl'), 'rb') as f:
#             self.seq_offsets = pickle.load(f)

#     def _load_user_data(self, uid):
#         self.data_file.seek(self.seq_offsets[uid])
#         line = self.data_file.readline()
#         try:
#             return json.loads(line)
#         except (json.JSONDecodeError, Exception):
#             return []

#     def __len__(self):
#         return len(self.seq_offsets)

#         # æ–°çš„ __getitem__ æ–¹æ³•
#     def __getitem__(self, uid):
#         # uid æ˜¯ dataloader ä¼ è¿‡æ¥çš„ç”¨æˆ·æ•´æ•°ç´¢å¼•
        
#         # 1. <<< è·å–ç”¨æˆ·é™æ€ç‰¹å¾ (æå¿«) >>>
#         # ç›´æ¥ä»å†…å­˜ä¸­çš„å­—å…¸é‡Œï¼Œç”¨æ•´æ•°ç´¢å¼• uid æ‹¿åˆ°é¢„å¤„ç†å¥½çš„ç‰¹å¾
#         user_static_feat_indexed = self.user_features.get(uid, {})
        
#         # 2. <<< è¯»å–åŸå§‹åºåˆ—æ•°æ® >>>
#         # æ³¨æ„ï¼š_load_user_data éœ€è¦ uid çš„åŸå§‹ IDï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåå‘æ˜ å°„
#         # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾ dataloader çš„ index å’Œæˆ‘ä»¬çš„ uid æ˜¯ä¸€è‡´çš„ã€‚
#         # å¦‚æœä¸ä¸€è‡´ï¼Œä½ éœ€è¦å»ºç«‹ä¸€ä¸ª uid_idx -> original_uid çš„æ˜ å°„ã€‚
#         # æš‚æ—¶æˆ‘ä»¬å…ˆå‡è®¾ä¼ è¿›æ¥çš„ uid å°±æ˜¯æ–‡ä»¶ä¸­çš„ç”¨æˆ·è¡Œå·ç´¢å¼•ã€‚
#         user_sequence = self._load_user_data(uid)
        
#         item_interactions = []
#         user_full_history_items = set()
#         for u, i, user_feat, item_feat, action_type, ts in user_sequence:
#             if i and item_feat:
#                 # æˆ‘ä»¬åªéœ€è¦ç‰©å“çš„åŸå§‹IDå’Œè¡Œä¸ºç±»å‹
#                 item_interactions.append({'id': i, 'action': action_type})
#                 # åŒæ—¶ï¼Œå°†ç‰©å“çš„æ•´æ•°ç´¢å¼•åŠ å…¥å†å²è®°å½•ï¼Œç”¨äºè´Ÿé‡‡æ ·æ’é™¤
#                 item_idx = self.item_original_id_to_idx.get(i)
#                 if item_idx is not None:
#                     user_full_history_items.add(item_idx)

#         full_history_tensor = torch.tensor(list(user_full_history_items), dtype=torch.long)
        
#         # 3. <<< å‡†å¤‡ç©ºçš„ Numpy æ•°ç»„ç”¨äºå¡«å…… >>>
#         # ç‰©å“IDåºåˆ— (å­˜çš„æ˜¯æ•´æ•°ç´¢å¼•)
#         seq_ids = np.zeros(self.maxlen, dtype=np.int32)
#         pos_ids = np.zeros(self.maxlen, dtype=np.int32)
#         # è¡Œä¸ºåºåˆ—
#         action_seq = np.zeros(self.maxlen, dtype=np.int32)
#         # ç‰©å“ç¨€ç–ç‰¹å¾åºåˆ— (å­˜çš„æ˜¯ç‰¹å¾å€¼çš„æ•´æ•°ç´¢å¼•)
#         seq_sparse_feat = np.zeros((self.maxlen, len(self.ITEM_SPARSE_FEAT_KEYS)), dtype=np.int32)
#         pos_sparse_feat = np.zeros((self.maxlen, len(self.ITEM_SPARSE_FEAT_KEYS)), dtype=np.int32)
#         # ç‰©å“å¤šæ¨¡æ€ç‰¹å¾åºåˆ— (å­˜çš„æ˜¯åŸå§‹ embedding)
#         seq_mm_feat = {fid: np.zeros((self.maxlen, dim), dtype=np.float32) for fid, dim in self.ITEM_EMB_FEAT.items()}
#         pos_mm_feat = {fid: np.zeros((self.maxlen, dim), dtype=np.float32) for fid, dim in self.ITEM_EMB_FEAT.items()}

#         if not item_interactions:
#             # å¦‚æœç”¨æˆ·æ²¡æœ‰äº¤äº’ï¼Œç›´æ¥è¿”å›ç©ºçš„å’Œé™æ€ç‰¹å¾
#             return self._package_tensors(seq_ids, pos_ids, action_seq, user_static_feat_indexed, 
#                                         seq_sparse_feat, pos_sparse_feat, seq_mm_feat, pos_mm_feat, 
#                                         full_history_tensor)

#         history = item_interactions[:-1]
#         targets = item_interactions[1:]
        
#         start_idx = max(0, len(history) - self.maxlen)
#         history = history[start_idx:]
#         targets = targets[start_idx:]
        
#         end_idx = len(history)
        
#         # 4. <<< å¾ªç¯å¡«å……åºåˆ— (æ ¸å¿ƒé€»è¾‘) >>>
#         for i in range(end_idx):
#             # --- å¤„ç†å†å²åºåˆ— ---
#             hist_item = history[i]
#             # è·å–ç‰©å“çš„æ•´æ•°ç´¢å¼•
#             hist_item_idx = self.item_original_id_to_idx.get(hist_item['id'])
#             if hist_item_idx is not None:
#                 seq_ids[i] = hist_item_idx
#                 action_seq[i] = hist_item['action'] if hist_item['action'] is not None else 0
                
#                 # ä»å†…å­˜ä¸­è·å–è¯¥ç‰©å“çš„é¢„å¤„ç†ç‰¹å¾ (æå¿«)
#                 item_features_indexed = self.item_features.get(hist_item_idx, {})
#                 # å¡«å……ç¨€ç–ç‰¹å¾
#                 for feat_idx, feat_key in enumerate(self.ITEM_SPARSE_FEAT_KEYS):
#                     seq_sparse_feat[i, feat_idx] = item_features_indexed.get(feat_key, 0)
#                 # å¡«å……å¤šæ¨¡æ€ç‰¹å¾
#                 for fid, dim in self.ITEM_EMB_FEAT.items():
#                     v = item_features_indexed.get(fid)
#                     if v is not None:
#                         a = np.asarray(v, dtype=np.float32).flatten()
#                         actual_len = min(len(a), dim)
#                         seq_mm_feat[fid][i, :actual_len] = a[:actual_len]

#             # --- å¤„ç†ç›®æ ‡åºåˆ— (é€»è¾‘åŒä¸Š) ---
#             target_item = targets[i]
#             target_item_idx = self.item_original_id_to_idx.get(target_item['id'])
#             if target_item_idx is not None:
#                 pos_ids[i] = target_item_idx
                
#                 item_features_indexed = self.item_features.get(target_item_idx, {})
#                 for feat_idx, feat_key in enumerate(self.ITEM_SPARSE_FEAT_KEYS):
#                     pos_sparse_feat[i, feat_idx] = item_features_indexed.get(feat_key, 0)
#                 for fid, dim in self.ITEM_EMB_FEAT.items():
#                     v = item_features_indexed.get(fid)
#                     if v is not None:
#                         a = np.asarray(v, dtype=np.float32).flatten()
#                         actual_len = min(len(a), dim)
#                         pos_mm_feat[fid][i, :actual_len] = a[:actual_len]
                
#         return self._package_tensors(seq_ids, pos_ids, action_seq, user_static_feat_indexed,
#                                     seq_sparse_feat, pos_sparse_feat, seq_mm_feat, pos_mm_feat,
#                                     full_history_tensor)
#     # æ–°å¢ä¸€ä¸ªè¾…åŠ©æ–¹æ³•ï¼Œç”¨äºå°† Numpy æ•°ç»„æ‰“åŒ…æˆ Torch å¼ é‡
#     def _package_tensors(self, seq_ids, pos_ids, action_seq, user_static_feat_indexed,
#                         seq_sparse_feat, pos_sparse_feat, seq_mm_feat, pos_mm_feat,
#                         full_history_tensor):
        
#         # ç”¨æˆ·é™æ€ç‰¹å¾
#         user_sparse_tensor = torch.tensor(
#             [user_static_feat_indexed.get(k, 0) for k in self.USER_SPARSE_FEAT_KEYS], 
#             dtype=torch.long
#         )

#         # åºåˆ—ç›¸å…³ç‰¹å¾
#         seq_ids_tensor = torch.tensor(seq_ids, dtype=torch.long)
#         pos_ids_tensor = torch.tensor(pos_ids, dtype=torch.long)
#         action_seq_tensor = torch.tensor(action_seq, dtype=torch.long)
#         seq_sparse_tensor = torch.tensor(seq_sparse_feat, dtype=torch.long)
#         pos_sparse_tensor = torch.tensor(pos_sparse_feat, dtype=torch.long)
        
#         seq_mm_tensors = {fid: torch.from_numpy(arr) for fid, arr in seq_mm_feat.items()}
#         pos_mm_tensors = {fid: torch.from_numpy(arr) for fid, arr in pos_mm_feat.items()}

#         return (seq_ids_tensor, pos_ids_tensor, action_seq_tensor, 
#                 user_sparse_tensor, seq_sparse_tensor, pos_sparse_tensor, 
#                 seq_mm_tensors, pos_mm_tensors, full_history_tensor)
#     # def _features_to_tensors(self, seq_ids, pos_ids, action_seq, seq_feat_list, pos_feat_list):
#     #     seq_ids_tensor = torch.tensor(seq_ids, dtype=torch.long)
#     #     pos_ids_tensor = torch.tensor(pos_ids, dtype=torch.long)
#     #     action_seq_tensor = torch.tensor(action_seq, dtype=torch.long)
#     #     seq_sparse_tensor = torch.tensor([[feat.get(str(fid), 0) for fid in self.ITEM_SPARSE_FEAT_KEYS] for feat in seq_feat_list], dtype=torch.long)
#     #     pos_sparse_tensor = torch.tensor([[feat.get(str(fid), 0) for fid in self.ITEM_SPARSE_FEAT_KEYS] for feat in pos_feat_list], dtype=torch.long)
#     #     seq_mm_tensors, pos_mm_tensors = {}, {}
#     #     for fid, dim in self.ITEM_EMB_FEAT.items():
#     #         seq_mm_tensors[fid] = torch.from_numpy(self._extract_mm_feat(seq_feat_list, fid, dim))
#     #         pos_mm_tensors[fid] = torch.from_numpy(self._extract_mm_feat(pos_feat_list, fid, dim))
#     #     return seq_ids_tensor, pos_ids_tensor, action_seq_tensor, seq_sparse_tensor, pos_sparse_tensor, seq_mm_tensors, pos_mm_tensors
    
#     def _extract_mm_feat(self, feat_list, fid, dim):
#         mm_np = np.zeros((len(feat_list), dim), dtype=np.float32)
#         for t, f in enumerate(feat_list):
#             v = f.get(str(fid))
#             if v is not None:
#                 a = np.asarray(v, dtype=np.float32).flatten()
#                 actual_len = min(len(a), dim)
#                 mm_np[t, :actual_len] = a[:actual_len]
#         return mm_np

#     def _init_feat_info(self):
#         feat_default_value = {}
#         feat_statistics = {}
#         feat_types = {
#             'user_sparse': ['103', '104', '105', '109'],
#             'item_sparse': ['100', '117', '111', '118', '101', '102', '119', '120', '114', '112', '121', '115', '122', '116'],
#             'item_emb': self.mm_emb_ids
#         }
#         all_sparse_ids = set(feat_types['user_sparse']) | set(feat_types['item_sparse'])
#         for feat_id in all_sparse_ids:
#             feat_default_value[feat_id] = 0
#             feat_statistics[feat_id] = len(self.indexer['f'].get(feat_id, {}))
#         return feat_default_value, feat_types, feat_statistics

#     def fill_missing_feat(self, feat, item_id):
#         if feat is None: feat = {}
#         filled_feat = feat.copy()
#         for k in self.ITEM_SPARSE_FEAT_KEYS:
#             if k not in filled_feat:
#                 filled_feat[k] = self.feature_default_value.get(k, 0)
#         return filled_feat

#     @staticmethod
#     # def collate_fn(batch):
#     #     (seq_ids, pos_ids, action_seqs, 
#     #      seq_sparse, pos_sparse, 
#     #      seq_mm, pos_mm, full_histories) = zip(*batch)

#     #     seq_ids, pos_ids, action_seqs, seq_sparse, pos_sparse = \
#     #         torch.stack(seq_ids), torch.stack(pos_ids), torch.stack(action_seqs), \
#     #         torch.stack(seq_sparse), torch.stack(pos_sparse)

#     #     seq_mm_batch, pos_mm_batch = {}, {}
#     #     if seq_mm[0]:
#     #         for fid in seq_mm[0].keys():
#     #             seq_mm_batch[fid] = torch.stack([x[fid] for x in seq_mm])
#     #             pos_mm_batch[fid] = torch.stack([x[fid] for x in pos_mm])
        
#     #     return (seq_ids, pos_ids, action_seqs, 
#     #             seq_sparse, pos_sparse, 
#     #             seq_mm_batch, pos_mm_batch,
#     #             list(full_histories))
#     # æ–°çš„ __getitem__ æ–¹æ³•
#     def __getitem__(self, uid):
#         # uid æ˜¯ dataloader ä¼ è¿‡æ¥çš„ç”¨æˆ·æ•´æ•°ç´¢å¼•
        
#         # 1. <<< è·å–ç”¨æˆ·é™æ€ç‰¹å¾ (æå¿«) >>>
#         # ç›´æ¥ä»å†…å­˜ä¸­çš„å­—å…¸é‡Œï¼Œç”¨æ•´æ•°ç´¢å¼• uid æ‹¿åˆ°é¢„å¤„ç†å¥½çš„ç‰¹å¾
#         user_static_feat_indexed = self.user_features.get(uid, {})
        
#         # 2. <<< è¯»å–åŸå§‹åºåˆ—æ•°æ® >>>
#         # æ³¨æ„ï¼š_load_user_data éœ€è¦ uid çš„åŸå§‹ IDï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåå‘æ˜ å°„
#         # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å‡è®¾ dataloader çš„ index å’Œæˆ‘ä»¬çš„ uid æ˜¯ä¸€è‡´çš„ã€‚
#         # å¦‚æœä¸ä¸€è‡´ï¼Œä½ éœ€è¦å»ºç«‹ä¸€ä¸ª uid_idx -> original_uid çš„æ˜ å°„ã€‚
#         # æš‚æ—¶æˆ‘ä»¬å…ˆå‡è®¾ä¼ è¿›æ¥çš„ uid å°±æ˜¯æ–‡ä»¶ä¸­çš„ç”¨æˆ·è¡Œå·ç´¢å¼•ã€‚
#         user_sequence = self._load_user_data(uid)
        
#         item_interactions = []
#         user_full_history_items = set()
#         for u, i, user_feat, item_feat, action_type, ts in user_sequence:
#             if i and item_feat:
#                 # æˆ‘ä»¬åªéœ€è¦ç‰©å“çš„åŸå§‹IDå’Œè¡Œä¸ºç±»å‹
#                 item_interactions.append({'id': i, 'action': action_type})
#                 # åŒæ—¶ï¼Œå°†ç‰©å“çš„æ•´æ•°ç´¢å¼•åŠ å…¥å†å²è®°å½•ï¼Œç”¨äºè´Ÿé‡‡æ ·æ’é™¤
#                 item_idx = self.item_original_id_to_idx.get(i)
#                 if item_idx is not None:
#                     user_full_history_items.add(item_idx)

#         full_history_tensor = torch.tensor(list(user_full_history_items), dtype=torch.long)
        
#         # 3. <<< å‡†å¤‡ç©ºçš„ Numpy æ•°ç»„ç”¨äºå¡«å…… >>>
#         # ç‰©å“IDåºåˆ— (å­˜çš„æ˜¯æ•´æ•°ç´¢å¼•)
#         seq_ids = np.zeros(self.maxlen, dtype=np.int32)
#         pos_ids = np.zeros(self.maxlen, dtype=np.int32)
#         # è¡Œä¸ºåºåˆ—
#         action_seq = np.zeros(self.maxlen, dtype=np.int32)
#         # ç‰©å“ç¨€ç–ç‰¹å¾åºåˆ— (å­˜çš„æ˜¯ç‰¹å¾å€¼çš„æ•´æ•°ç´¢å¼•)
#         seq_sparse_feat = np.zeros((self.maxlen, len(self.ITEM_SPARSE_FEAT_KEYS)), dtype=np.int32)
#         pos_sparse_feat = np.zeros((self.maxlen, len(self.ITEM_SPARSE_FEAT_KEYS)), dtype=np.int32)
#         # ç‰©å“å¤šæ¨¡æ€ç‰¹å¾åºåˆ— (å­˜çš„æ˜¯åŸå§‹ embedding)
#         seq_mm_feat = {fid: np.zeros((self.maxlen, dim), dtype=np.float32) for fid, dim in self.ITEM_EMB_FEAT.items()}
#         pos_mm_feat = {fid: np.zeros((self.maxlen, dim), dtype=np.float32) for fid, dim in self.ITEM_EMB_FEAT.items()}

#         if not item_interactions:
#             # å¦‚æœç”¨æˆ·æ²¡æœ‰äº¤äº’ï¼Œç›´æ¥è¿”å›ç©ºçš„å’Œé™æ€ç‰¹å¾
#             return self._package_tensors(seq_ids, pos_ids, action_seq, user_static_feat_indexed, 
#                                         seq_sparse_feat, pos_sparse_feat, seq_mm_feat, pos_mm_feat, 
#                                         full_history_tensor)

#         history = item_interactions[:-1]
#         targets = item_interactions[1:]
        
#         start_idx = max(0, len(history) - self.maxlen)
#         history = history[start_idx:]
#         targets = targets[start_idx:]
        
#         end_idx = len(history)
        
#         # 4. <<< å¾ªç¯å¡«å……åºåˆ— (æ ¸å¿ƒé€»è¾‘) >>>
#         for i in range(end_idx):
#             # --- å¤„ç†å†å²åºåˆ— ---
#             hist_item = history[i]
#             # è·å–ç‰©å“çš„æ•´æ•°ç´¢å¼•
#             hist_item_idx = self.item_original_id_to_idx.get(hist_item['id'])
#             if hist_item_idx is not None:
#                 seq_ids[i] = hist_item_idx
#                 action_seq[i] = hist_item['action'] if hist_item['action'] is not None else 0
                
#                 # ä»å†…å­˜ä¸­è·å–è¯¥ç‰©å“çš„é¢„å¤„ç†ç‰¹å¾ (æå¿«)
#                 item_features_indexed = self.item_features.get(hist_item_idx, {})
#                 # å¡«å……ç¨€ç–ç‰¹å¾
#                 for feat_idx, feat_key in enumerate(self.ITEM_SPARSE_FEAT_KEYS):
#                     seq_sparse_feat[i, feat_idx] = item_features_indexed.get(feat_key, 0)
#                 # å¡«å……å¤šæ¨¡æ€ç‰¹å¾
#                 for fid, dim in self.ITEM_EMB_FEAT.items():
#                     v = item_features_indexed.get(fid)
#                     if v is not None:
#                         a = np.asarray(v, dtype=np.float32).flatten()
#                         actual_len = min(len(a), dim)
#                         seq_mm_feat[fid][i, :actual_len] = a[:actual_len]

#             # --- å¤„ç†ç›®æ ‡åºåˆ— (é€»è¾‘åŒä¸Š) ---
#             target_item = targets[i]
#             target_item_idx = self.item_original_id_to_idx.get(target_item['id'])
#             if target_item_idx is not None:
#                 pos_ids[i] = target_item_idx
                
#                 item_features_indexed = self.item_features.get(target_item_idx, {})
#                 for feat_idx, feat_key in enumerate(self.ITEM_SPARSE_FEAT_KEYS):
#                     pos_sparse_feat[i, feat_idx] = item_features_indexed.get(feat_key, 0)
#                 for fid, dim in self.ITEM_EMB_FEAT.items():
#                     v = item_features_indexed.get(fid)
#                     if v is not None:
#                         a = np.asarray(v, dtype=np.float32).flatten()
#                         actual_len = min(len(a), dim)
#                         pos_mm_feat[fid][i, :actual_len] = a[:actual_len]
                
#         return self._package_tensors(seq_ids, pos_ids, action_seq, user_static_feat_indexed,
#                                     seq_sparse_feat, pos_sparse_feat, seq_mm_feat, pos_mm_feat,
#                                     full_history_tensor)

# class MyTestDataset(MyDataset):
#     """
#     æµ‹è¯•æ•°æ®é›†ï¼Œç”¨äºç”Ÿæˆé¢„æµ‹ç»“æœã€‚
#     """
#     def _load_data_and_offsets(self):
#         """
#         åŠ è½½é¢„æµ‹ç”¨çš„ç”¨æˆ·åºåˆ—æ•°æ®å’Œåç§»é‡ã€‚
#         """
#         predict_file = self.data_dir / "predict_seq.jsonl"
#         offsets_file = self.data_dir / 'predict_seq_offsets.pkl'
        
#         if not predict_file.exists() or not offsets_file.exists():
#             raise FileNotFoundError(f"é¢„æµ‹æ–‡ä»¶ {predict_file} æˆ– {offsets_file} ä¸å­˜åœ¨ã€‚")
            
#         self.data_file = open(predict_file, 'rb')
#         with open(offsets_file, 'rb') as f:
#             self.seq_offsets = pickle.load(f)

#     # â­ [æ ¸å¿ƒä¿®æ”¹] ä¸ºæµ‹è¯•é›†é‡å†™ __getitem__ æ–¹æ³•
#     def __getitem__(self, uid):
#         user_sequence = self._load_user_data(uid)
        
#         item_interactions = []
#         user_full_history_items = set()
#         for u, i, user_feat, item_feat, action_type, ts in user_sequence:
#             if i and item_feat:
#                 item_interactions.append({'id': i, 'feat': item_feat, 'action': action_type})
#                 user_full_history_items.add(i)

#         full_history_tensor = torch.tensor(list(user_full_history_items), dtype=torch.long)
        
#         seq_ids = np.zeros(self.maxlen, dtype=np.int32)
#         action_seq = np.zeros(self.maxlen, dtype=np.int32)
#         seq_feat = [self.feature_default_value.copy() for _ in range(self.maxlen)]
        
#         # æµ‹è¯•æ—¶æ²¡æœ‰ç›®æ ‡ï¼Œæ‰€ä»¥ pos åºåˆ—ç”¨ 0 å¡«å……
#         pos_ids = np.zeros(self.maxlen, dtype=np.int32)
#         pos_feat = [self.feature_default_value.copy() for _ in range(self.maxlen)]

#         if not item_interactions:
#             tensors = self._features_to_tensors(seq_ids, pos_ids, action_seq, seq_feat, pos_feat)
#             return (*tensors, full_history_tensor)

#         # æµ‹è¯•é€»è¾‘: history æ˜¯å®Œæ•´çš„ç”¨æˆ·äº¤äº’åºåˆ—
#         history = item_interactions
        
#         start_idx = max(0, len(history) - self.maxlen)
#         history = history[start_idx:]
        
#         end_idx = len(history)
        
#         for i in range(end_idx):
#             hist_item = history[i]
#             seq_ids[i] = hist_item['id']
#             seq_feat[i] = self.fill_missing_feat(hist_item['feat'], hist_item['id'])
#             action_seq[i] = hist_item['action'] if hist_item['action'] is not None else 0
            
#         tensors = self._features_to_tensors(seq_ids, pos_ids, action_seq, seq_feat, pos_feat)
#         return (*tensors, full_history_tensor)

#     @staticmethod
#     def collate_fn(batch):
#         """
#         ä¸ºæµ‹è¯•é›†å®šåˆ¶çš„ collate_fnã€‚
#         å®ƒè´Ÿè´£å°† __getitem__ çš„è¾“å‡ºæ‰“åŒ…æˆä¸è®­ç»ƒæ—¶æ ¼å¼ä¸€è‡´çš„å…ƒç»„ï¼Œ
#         ä»¥ä¾¿éªŒè¯å’Œæ¨ç†ä»£ç å¯ä»¥ç»Ÿä¸€å¤„ç†ã€‚
#         """
#         # è§£åŒ… __getitem__ è¿”å›çš„æ‰€æœ‰å…ƒç´ 
#         (seq_ids, pos_ids, action_seqs, 
#          seq_sparse, pos_sparse, 
#          seq_mm, pos_mm, full_histories) = zip(*batch)

#         # å †å åŸºæœ¬å¼ é‡
#         seq_ids = torch.stack(seq_ids, dim=0)
#         pos_ids = torch.stack(pos_ids, dim=0) # pos_ids åœ¨æµ‹è¯•æ—¶ä½œä¸º ground truth æˆ–å ä½ç¬¦
#         action_seqs = torch.stack(action_seqs, dim=0)
#         seq_sparse = torch.stack(seq_sparse, dim=0)
#         pos_sparse = torch.stack(pos_sparse, dim=0)

#         # å †å å¤šæ¨¡æ€ç‰¹å¾å­—å…¸
#         seq_mm_batch, pos_mm_batch = {}, {}
#         if seq_mm and seq_mm[0]:
#             for fid in seq_mm[0].keys():
#                 seq_mm_batch[fid] = torch.stack([x[fid] for x in seq_mm])
#                 pos_mm_batch[fid] = torch.stack([x[fid] for x in pos_mm])
        
#         # è¿”å›æ‰€æœ‰æ‰“åŒ…å¥½çš„æ•°æ®
#         return (seq_ids, pos_ids, action_seqs, 
#                 seq_sparse, pos_sparse, 
#                 seq_mm_batch, pos_mm_batch,
#                 list(full_histories))



    



def save_emb(emb, save_path):
    """
    å°†Embeddingä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶

    Args:
        emb: è¦ä¿å­˜çš„Embeddingï¼Œå½¢çŠ¶ä¸º [num_points, num_dimensions]
        save_path: ä¿å­˜è·¯å¾„
    """
    num_points = emb.shape[0]  # æ•°æ®ç‚¹æ•°é‡
    num_dimensions = emb.shape[1]  # å‘é‡çš„ç»´åº¦
    print(f'saving {save_path}')
    with open(Path(save_path), 'wb') as f:
        f.write(struct.pack('II', num_points, num_dimensions))
        emb.tofile(f)


def load_mm_emb(mm_path, feat_ids):
    """
    åŠ è½½å¤šæ¨¡æ€ç‰¹å¾Embedding

    Args:
        mm_path: å¤šæ¨¡æ€ç‰¹å¾Embeddingè·¯å¾„
        feat_ids: è¦åŠ è½½çš„å¤šæ¨¡æ€ç‰¹å¾IDåˆ—è¡¨

    Returns:
        mm_emb_dict: å¤šæ¨¡æ€ç‰¹å¾Embeddingå­—å…¸ï¼Œkeyä¸ºç‰¹å¾IDï¼Œvalueä¸ºç‰¹å¾Embeddingå­—å…¸ï¼ˆkeyä¸ºitem IDï¼Œvalueä¸ºEmbeddingï¼‰
    """
    SHAPE_DICT = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
    mm_emb_dict = {}
    for feat_id in tqdm(feat_ids, desc='Loading mm_emb'):
        shape = SHAPE_DICT[feat_id]
        emb_dict = {}
        if feat_id != '81':
            try:
                base_path = Path(mm_path, f'emb_{feat_id}_{shape}')
                for json_file in base_path.glob('*.json'):
                    with open(json_file, 'r', encoding='utf-8') as file:
                        for line in file:
                            data_dict_origin = json.loads(line.strip())
                            insert_emb = data_dict_origin['emb']
                            if isinstance(insert_emb, list):
                                insert_emb = np.array(insert_emb, dtype=np.float32)
                            data_dict = {data_dict_origin['anonymous_cid']: insert_emb}
                            emb_dict.update(data_dict)
            except Exception as e:
                print(f"transfer error: {e}")
        if feat_id == '81':
            with open(Path(mm_path, f'emb_{feat_id}_{shape}.pkl'), 'rb') as f:
                emb_dict = pickle.load(f)
        mm_emb_dict[feat_id] = emb_dict
        print(f'Loaded #{feat_id} mm_emb')
    return mm_emb_dict
import json
import pickle
import numpy as np
import torch
from pathlib import Path

class MyDataset(torch.utils.data.Dataset):
    def __init__(self, data_dir, args):
        super().__init__()
        self.data_dir = Path(data_dir)
        self.maxlen = args.maxlen
        self.mm_emb_ids = args.mm_emb_id # å¤šæ¨¡æ€ç‰¹å¾ID

        # --- 1. åŠ è½½æ‰€æœ‰é¢„å¤„ç†å’Œå®˜æ–¹æ–‡ä»¶ ---
        print("ğŸ’¾ æ­£åœ¨åŠ è½½å®˜æ–¹å’Œé¢„å¤„ç†æ–‡ä»¶...")
        with open(self.data_dir / 'indexer.pkl', 'rb') as f:
            self.indexer = pickle.load(f)
        with open(self.data_dir / 'user_features.pkl', 'rb') as f:
            self.user_features = pickle.load(f)
        with open(self.data_dir / 'item_features.pkl', 'rb') as f:
            self.item_features = pickle.load(f)
        with open(self.data_dir / 'seq_offsets.pkl', 'rb') as f:
            self.seq_offsets = pickle.load(f)
        print("âœ… æ–‡ä»¶åŠ è½½å®Œæ¯•!")
        
        # æ‰“å¼€åºåˆ—æ–‡ä»¶ä»¥å¤‡è¯»å–
        self.data_file = open(self.data_dir / "seq.jsonl", 'r', encoding='utf-8')

        # --- 2. è®¾ç½®å…ƒä¿¡æ¯å’Œç‰¹å¾é”® ---
        self.usernum = len(self.indexer['u'])
        self.itemnum = len(self.indexer['i'])
        # DataLoader çš„ç´¢å¼•æ˜¯ä»0å¼€å§‹çš„ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ˜ å°„åˆ°å®˜æ–¹çš„ user re-id
        # æˆ‘ä»¬å‡è®¾ seq_offsets.pkl çš„é¡ºåºå¯¹åº”äº† user re-id ä»1åˆ°N
        # å¦‚æœä¸æ˜¯ï¼Œéœ€è¦æ ¹æ® indexer['u'] åˆ›å»ºæ›´å¤æ‚çš„æ˜ å°„
        self.dataloader_idx_to_user_reid = list(range(1, len(self.seq_offsets) + 1))


        self.USER_SPARSE_KEYS = ['103', '104', '105', '109']
        self.USER_ARRAY_KEYS = ['205', '207']
        self.MAX_ARRAY_LENGTH = 10
        self.ITEM_SPARSE_KEYS = ['100', '117', '111', '118', '101', '102', '119', '120', '114', '112', '121', '115', '122', '116']
        
        self.feat_statistics = {
            feat_id: len(feat_map) for feat_id, feat_map in self.indexer['f'].items()
        }
        self.feature_types = {
            'user_sparse': self.USER_SPARSE_KEYS,
            'user_array': self.USER_ARRAY_KEYS,
            'item_sparse': self.ITEM_SPARSE_KEYS,
            'item_emb': self.mm_emb_ids,
        }
        # (å¤šæ¨¡æ€ç›¸å…³çš„å®šä¹‰ï¼Œå¦‚æœé€‚ç”¨çš„è¯)
        EMB_SHAPE_DICT = {"81": 32} # å‡è®¾åªæœ‰81æ˜¯å¤šæ¨¡æ€
        self.ITEM_EMB_FEAT_DIMS = {k: EMB_SHAPE_DICT[k] for k in self.mm_emb_ids if k in EMB_SHAPE_DICT}

    def _load_user_data(self, dataloader_idx):
        offset = self.seq_offsets[dataloader_idx]
        self.data_file.seek(offset)
        line = self.data_file.readline()
        return json.loads(line)

    def __len__(self):
        return len(self.seq_offsets)

    def __getitem__(self, dataloader_idx):
        user_reid = self.dataloader_idx_to_user_reid[dataloader_idx]
        
        # 1. è·å–ç”¨æˆ·é™æ€ç‰¹å¾ (ä»é¢„å¤„ç†æ–‡ä»¶ä¸­)
        user_static_feat = self.user_features.get(user_reid, {})

        # 2. è¯»å–ç”¨æˆ·è¡Œä¸ºåºåˆ—
        user_sequence = self._load_user_data(dataloader_idx)
        
        item_interactions = []
        user_full_history_items = set()
        for record in user_sequence:
            # åªå¤„ç†ç‰©å“äº¤äº’è®°å½•
            if record[1] is not None:
                item_id = record[1]
                action_type = record[4]
                item_interactions.append({'id': item_id, 'action': action_type})
                user_full_history_items.add(item_id)

        full_history_tensor = torch.tensor(list(user_full_history_items), dtype=torch.long)
        
        # 3. å‡†å¤‡ç©ºçš„ Numpy æ•°ç»„
        seq_ids = np.zeros(self.maxlen, dtype=np.int32)
        pos_ids = np.zeros(self.maxlen, dtype=np.int32)
        action_seq = np.zeros(self.maxlen, dtype=np.int32)
        seq_sparse_feat = np.zeros((self.maxlen, len(self.ITEM_SPARSE_KEYS)), dtype=np.int32)
        pos_sparse_feat = np.zeros((self.maxlen, len(self.ITEM_SPARSE_KEYS)), dtype=np.int32)
        # (å¤„ç†å¤šæ¨¡æ€ï¼Œå¦‚æœéœ€è¦)
        # seq_mm_feat = ...

        # 4. å¡«å……åºåˆ—
        history = item_interactions[:-1]
        targets = item_interactions[1:]
        start_idx = max(0, len(history) - self.maxlen)
        history = history[start_idx:]
        targets = targets[start_idx:]
        end_idx = len(history)
        
        for i in range(end_idx):
            # å¤„ç†å†å²åºåˆ—
            hist_item_id = history[i]['id']
            seq_ids[i] = hist_item_id
            action_seq[i] = history[i]['action'] if history[i]['action'] is not None else 0
            
            item_features = self.item_features.get(hist_item_id, {})
            for feat_idx, feat_key in enumerate(self.ITEM_SPARSE_KEYS):
                # å€¼å·²ç»æ˜¯ re-idï¼Œç›´æ¥ä½¿ç”¨
                seq_sparse_feat[i, feat_idx] = item_features.get(feat_key, 0)

            # å¤„ç†ç›®æ ‡åºåˆ—
            target_item_id = targets[i]['id']
            pos_ids[i] = target_item_id
            item_features = self.item_features.get(target_item_id, {})
            for feat_idx, feat_key in enumerate(self.ITEM_SPARSE_KEYS):
                pos_sparse_feat[i, feat_idx] = item_features.get(feat_key, 0)
        
        # 5. å°†æ‰€æœ‰æ•°æ®æ‰“åŒ…æˆå¼ é‡
        return self._package_tensors(seq_ids, pos_ids, action_seq, user_static_feat,
                                     seq_sparse_feat, pos_sparse_feat, full_history_tensor)

    def _package_tensors(self, seq_ids, pos_ids, action_seq, user_static_feat,
                         seq_sparse_feat, pos_sparse_feat, full_history_tensor):
        
        user_sparse_tensor = torch.tensor([user_static_feat.get(k, 0) for k in self.USER_SPARSE_KEYS], dtype=torch.long)
        user_array_tensors = {
            k: torch.tensor(user_static_feat.get(k, [0] * self.MAX_ARRAY_LENGTH), dtype=torch.long)
            for k in self.USER_ARRAY_KEYS
        }

        seq_ids_tensor = torch.tensor(seq_ids, dtype=torch.long)
        pos_ids_tensor = torch.tensor(pos_ids, dtype=torch.long)
        action_seq_tensor = torch.tensor(action_seq, dtype=torch.long)
        seq_sparse_tensor = torch.tensor(seq_sparse_feat, dtype=torch.long)
        pos_sparse_tensor = torch.tensor(pos_sparse_feat, dtype=torch.long)
        
        # (å¤šæ¨¡æ€ç‰¹å¾æ‰“åŒ…ï¼Œå¦‚æœéœ€è¦)
        seq_mm_tensors, pos_mm_tensors = {}, {}

        return (seq_ids_tensor, pos_ids_tensor, action_seq_tensor, 
                user_sparse_tensor, user_array_tensors,
                seq_sparse_tensor, pos_sparse_tensor, 
                seq_mm_tensors, pos_mm_tensors, full_history_tensor)

    @staticmethod
    def collate_fn(batch):
        # è¿™ä¸ª collate_fn ä¸æˆ‘ä»¬ä¹‹å‰çš„ç‰ˆæœ¬ä¸€è‡´ï¼Œå¯ä»¥æ­£ç¡®å¤„ç†
        (seq_ids, pos_ids, action_seqs, 
         user_sparse, user_arrays,
         seq_sparse, pos_sparse, 
         seq_mm, pos_mm, full_histories) = zip(*batch)

        seq_ids, pos_ids, action_seqs, user_sparse, seq_sparse, pos_sparse = \
            torch.stack(seq_ids), torch.stack(pos_ids), torch.stack(action_seqs), \
            torch.stack(user_sparse), torch.stack(seq_sparse), torch.stack(pos_sparse)

        user_arrays_batch = {}
        if user_arrays and user_arrays[0]:
            for fid in user_arrays[0].keys():
                user_arrays_batch[fid] = torch.stack([x[fid] for x in user_arrays])
        
        # (å¤šæ¨¡æ€ collate é€»è¾‘)
        seq_mm_batch, pos_mm_batch = {}, {}

        return (seq_ids, pos_ids, action_seqs, 
                user_sparse, user_arrays_batch,
                seq_sparse, pos_sparse, 
                seq_mm_batch, pos_mm_batch,
                list(full_histories))