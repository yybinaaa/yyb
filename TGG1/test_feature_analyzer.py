#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾åˆ†æå™¨æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•ç‰¹å¾åˆ†æå™¨çš„åŸºæœ¬åŠŸèƒ½ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•ã€‚
"""

import json
import pickle
import numpy as np
from pathlib import Path
import tempfile
import shutil


def create_test_data(test_dir: Path):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    print("æ­£åœ¨åˆ›å»ºæµ‹è¯•æ•°æ®...")
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir.mkdir(exist_ok=True)
    
    # 1. åˆ›å»ºç‰©å“ç‰¹å¾å­—å…¸
    item_feat_dict = {}
    for i in range(1000):  # 1000ä¸ªç‰©å“
        item_feat_dict[str(i)] = {
            '100': np.random.randint(0, 1000),  # ç¨€ç–ç‰¹å¾
            '101': np.random.randint(0, 500),
            '102': np.random.randint(0, 200),
            '117': np.random.randint(0, 100),
            '111': np.random.randint(0, 300),
            '118': np.random.randint(0, 150),
            '119': np.random.randint(0, 400),
            '120': np.random.randint(0, 250),
            '114': np.random.randint(0, 600),
            '112': np.random.randint(0, 350),
            '121': np.random.randint(0, 450),
            '115': np.random.randint(0, 280),
            '122': np.random.randint(0, 320),
            '116': np.random.randint(0, 180)
        }
    
    # ä¿å­˜ç‰©å“ç‰¹å¾å­—å…¸
    with open(test_dir / "item_feat_dict.json", 'w', encoding='utf-8') as f:
        json.dump(item_feat_dict, f, ensure_ascii=False, indent=2)
    
    # 2. åˆ›å»ºç´¢å¼•æ–‡ä»¶
    indexer = {
        'i': {f'item_{i}': i for i in range(1000)},  # ç‰©å“ç´¢å¼•
        'u': {f'user_{i}': i for i in range(100)},   # ç”¨æˆ·ç´¢å¼•
        'f': {  # ç‰¹å¾ç´¢å¼•
            '100': {f'feat_100_{i}': i for i in range(1000)},
            '101': {f'feat_101_{i}': i for i in range(500)},
            '102': {f'feat_102_{i}': i for i in range(200)},
            '103': {f'feat_103_{i}': i for i in range(100)},
            '104': {f'feat_104_{i}': i for i in range(80)},
            '105': {f'feat_105_{i}': i for i in range(120)},
            '106': {f'feat_106_{i}': i for i in range(50)},
            '107': {f'feat_107_{i}': i for i in range(60)},
            '108': {f'feat_108_{i}': i for i in range(40)},
            '109': {f'feat_109_{i}': i for i in range(90)},
            '110': {f'feat_110_{i}': i for i in range(70)},
            '111': {f'feat_111_{i}': i for i in range(300)},
            '112': {f'feat_112_{i}': i for i in range(350)},
            '113': {f'feat_113_{i}': i for i in range(400)},
            '114': {f'feat_114_{i}': i for i in range(600)},
            '115': {f'feat_115_{i}': i for i in range(280)},
            '116': {f'feat_116_{i}': i for i in range(180)},
            '117': {f'feat_117_{i}': i for i in range(100)},
            '118': {f'feat_118_{i}': i for i in range(150)},
            '119': {f'feat_119_{i}': i for i in range(400)},
            '120': {f'feat_120_{i}': i for i in range(250)},
            '121': {f'feat_121_{i}': i for i in range(450)},
            '122': {f'feat_122_{i}': i for i in range(320)}
        }
    }
    
    # ä¿å­˜ç´¢å¼•æ–‡ä»¶
    with open(test_dir / "indexer.pkl", 'wb') as f:
        pickle.dump(indexer, f)
    
    # 3. åˆ›å»ºå¤šæ¨¡æ€ç‰¹å¾ç›®å½•ï¼ˆæ¨¡æ‹Ÿï¼‰
    mm_dir = test_dir / "creative_emb"
    mm_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿçš„embeddingæ–‡ä»¶
    embedding_shapes = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
    
    for feat_id, shape in embedding_shapes.items():
        if feat_id == '81':
            # åˆ›å»ºpickleæ ¼å¼çš„embedding
            emb_dict = {f'item_{i}': np.random.randn(shape).astype(np.float32) for i in range(100)}
            with open(mm_dir / f'emb_{feat_id}_{shape}.pkl', 'wb') as f:
                pickle.dump(emb_dict, f)
        else:
            # åˆ›å»ºjsonæ ¼å¼çš„embeddingç›®å½•
            emb_subdir = mm_dir / f'emb_{feat_id}_{shape}'
            emb_subdir.mkdir(exist_ok=True)
            
            # åˆ›å»ºå‡ ä¸ªjsonæ–‡ä»¶
            for batch in range(3):
                emb_data = []
                for i in range(100):
                    emb_data.append({
                        'anonymous_cid': f'item_{batch * 100 + i}',
                        'emb': np.random.randn(shape).astype(np.float32).tolist()
                    })
                
                with open(emb_subdir / f'batch_{batch}.json', 'w', encoding='utf-8') as f:
                    for item in emb_data:
                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ“ æµ‹è¯•æ•°æ®å·²åˆ›å»ºåœ¨: {test_dir}")
    return test_dir


def test_quick_analyzer(test_dir: Path):
    """æµ‹è¯•å¿«é€Ÿç‰¹å¾åˆ†æå™¨"""
    print("\n" + "="*50)
    print("æµ‹è¯•å¿«é€Ÿç‰¹å¾åˆ†æå™¨")
    print("="*50)
    
    try:
        # å¯¼å…¥å¿«é€Ÿåˆ†æå™¨
        from quick_feature_analyzer import load_feature_data, define_feature_types, analyze_feature_dimensions, generate_dimension_summary
        
        # åŠ è½½æ•°æ®
        data = load_feature_data(str(test_dir))
        
        # å®šä¹‰ç‰¹å¾ç±»å‹
        feature_types = define_feature_types()
        
        # åˆ†æç‰¹å¾ç»´åº¦
        analysis_result = analyze_feature_dimensions(data, feature_types)
        
        # ç”Ÿæˆæ‘˜è¦
        summary = generate_dimension_summary(analysis_result, feature_types)
        
        print("âœ“ å¿«é€Ÿç‰¹å¾åˆ†æå™¨æµ‹è¯•æˆåŠŸï¼")
        print("\nç”Ÿæˆçš„æ‘˜è¦é¢„è§ˆ:")
        print(summary[:500] + "..." if len(summary) > 500 else summary)
        
        return True
        
    except Exception as e:
        print(f"âœ— å¿«é€Ÿç‰¹å¾åˆ†æå™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_full_analyzer(test_dir: Path):
    """æµ‹è¯•å®Œæ•´ç‰ˆç‰¹å¾åˆ†æå™¨"""
    print("\n" + "="*50)
    print("æµ‹è¯•å®Œæ•´ç‰ˆç‰¹å¾åˆ†æå™¨")
    print("="*50)
    
    try:
        # å¯¼å…¥å®Œæ•´ç‰ˆåˆ†æå™¨
        from feature_dimension_generator import FeatureDimensionGenerator
        
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        generator = FeatureDimensionGenerator(str(test_dir))
        
        # è¿è¡Œåˆ†æ
        analysis_result, summary_df = generator.run_full_analysis("test_output")
        
        print("âœ“ å®Œæ•´ç‰ˆç‰¹å¾åˆ†æå™¨æµ‹è¯•æˆåŠŸï¼")
        print(f"è¾“å‡ºç›®å½•: test_output")
        print(f"åˆ†æç»“æœåŒ…å« {len(analysis_result)} ä¸ªéƒ¨åˆ†")
        print(f"æ‘˜è¦DataFrameå½¢çŠ¶: {summary_df.shape}")
        
        return True
        
    except Exception as e:
        print(f"âœ— å®Œæ•´ç‰ˆç‰¹å¾åˆ†æå™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•ç‰¹å¾åˆ†æå™¨...")
    
    # åˆ›å»ºä¸´æ—¶æµ‹è¯•ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        test_dir = Path(temp_dir) / "test_data"
        
        try:
            # 1. åˆ›å»ºæµ‹è¯•æ•°æ®
            create_test_data(test_dir)
            
            # 2. æµ‹è¯•å¿«é€Ÿåˆ†æå™¨
            quick_success = test_quick_analyzer(test_dir)
            
            # 3. æµ‹è¯•å®Œæ•´ç‰ˆåˆ†æå™¨
            full_success = test_full_analyzer(test_dir)
            
            # 4. è¾“å‡ºæµ‹è¯•ç»“æœ
            print("\n" + "="*50)
            print("æµ‹è¯•ç»“æœæ€»ç»“")
            print("="*50)
            print(f"å¿«é€Ÿç‰¹å¾åˆ†æå™¨: {'âœ“ é€šè¿‡' if quick_success else 'âœ— å¤±è´¥'}")
            print(f"å®Œæ•´ç‰ˆç‰¹å¾åˆ†æå™¨: {'âœ“ é€šè¿‡' if full_success else 'âœ— å¤±è´¥'}")
            
            if quick_success and full_success:
                print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç‰¹å¾åˆ†æå™¨å·¥ä½œæ­£å¸¸ã€‚")
            else:
                print("\nâš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
                
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
