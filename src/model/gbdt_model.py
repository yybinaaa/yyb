class GBDTClassifier:
    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1, min_samples_split=2):
        self.n_estimators = n_estimators           # å¼±å­¦ä¹ å™¨æ•°é‡
        self.max_depth = max_depth                 # æ ‘çš„æœ€å¤§æ·±åº¦
        self.learning_rate = learning_rate         # å­¦ä¹ ç‡ï¼ˆæ­¥é•¿ï¼‰
        self.min_samples_split = min_samples_split # åˆ†è£‚æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
        self.trees = []                            # å­˜å‚¨æ¯æ£µæ ‘
        self.init_pred = 0                         # åˆå§‹é¢„æµ‹å€¼ï¼ˆlog oddsï¼‰

    def _log_loss_gradient(self, y_true, y_pred):
        """è®¡ç®— log loss çš„è´Ÿæ¢¯åº¦ï¼ˆå³ pseudo-residualsï¼‰"""
        return y_true - 1 / (1 + np.exp(-y_pred))

    def _fit_tree(self, X, residuals, depth=0, min_samples_split=2):
        """é€’å½’æ„é€ ä¸€æ£µå›å½’æ ‘æ¥æ‹Ÿåˆæ®‹å·®"""
        if len(X) < min_samples_split or depth >= self.max_depth or np.var(residuals) < 1e-6:
            return {'value': np.mean(residuals), 'is_leaf': True}

        best_gain = -np.inf
        best_split = None
        best_left_mask = None
        m, n = X.shape

        for feat_idx in range(n):
            thresholds = np.unique(X[:, feat_idx])
            for thresh in thresholds:
                left_mask = X[:, feat_idx] <= thresh
                if np.sum(left_mask) == 0 or np.sum(~left_mask) == 0:
                    continue

                gain = self._compute_gain(residuals, left_mask)
                if gain > best_gain:
                    best_gain = gain
                    best_split = (feat_idx, thresh)
                    best_left_mask = left_mask

        if best_gain == -np.inf:
            return {'value': np.mean(residuals), 'is_leaf': True}

        feat_idx, thresh = best_split
        left_mask = best_left_mask

        left_tree = self._fit_tree(
            X[left_mask], residuals[left_mask], 
            depth + 1, min_samples_split
        )
        right_tree = self._fit_tree(
            X[~left_mask], residuals[~left_mask], 
            depth + 1, min_samples_split
        )

        return {
            'is_leaf': False,
            'feature': feat_idx,
            'threshold': thresh,
            'left': left_tree,
            'right': right_tree
        }

    def _compute_gain(self, residuals, left_mask):
        """è®¡ç®—åˆ†è£‚å¢ç›Šï¼ˆåŸºäºæ–¹å·®å‡å°‘ï¼‰"""
        var_full = np.var(residuals) * len(residuals)
        var_left = np.var(residuals[left_mask]) * np.sum(left_mask)
        var_right = np.var(residuals[~left_mask]) * np.sum(~left_mask)
        return var_full - var_left - var_right

    def _predict_tree(self, tree, x):
        """å•æ£µæ ‘é¢„æµ‹"""
        if tree['is_leaf']:
            return tree['value']
        feat_idx = tree['feature']
        thresh = tree['threshold']
        if x[feat_idx] <= thresh:
            return self._predict_tree(tree['left'], x)
        else:
            return self._predict_tree(tree['right'], x)

    def fit(self, X, y):
        """è®­ç»ƒ GBDT æ¨¡å‹"""
        X = np.array(X)
        y = np.array(y)
        
        # åˆå§‹åŒ–ä¸º log(odds)ï¼Œè‹¥æ— å…ˆéªŒå¯è®¾ä¸º 0
        self.init_pred = 0.0
        F = np.full(len(y), self.init_pred)

        self.trees = []

        for _ in range(self.n_estimators):
            # 1. è®¡ç®—è´Ÿæ¢¯åº¦ï¼ˆä¼ªæ®‹å·®ï¼‰
            residuals = self._log_loss_gradient(y, F)

            # 2. è®­ç»ƒä¸€æ£µå›å½’æ ‘æ¥æ‹Ÿåˆæ®‹å·®
            tree = self._fit_tree(X, residuals, depth=0, min_samples_split=self.min_samples_split)
            self.trees.append(tree)

            # 3. é¢„æµ‹æ‰€æœ‰æ ·æœ¬çš„æ®‹å·®ä¿®æ­£å€¼
            updates = np.array([self._predict_tree(tree, x) for x in X])

            # 4. æ›´æ–°å½“å‰æ¨¡å‹è¾“å‡ºï¼ˆå¸¦å­¦ä¹ ç‡ï¼‰
            F += self.learning_rate * updates

    def predict_proba(self, X):
        """é¢„æµ‹è´­ä¹°æ¦‚ç‡"""
        X = np.array(X)
        # åˆå§‹é¢„æµ‹ + æ‰€æœ‰æ ‘çš„ç´¯åŠ æ›´æ–°
        F = np.full(len(X), self.init_pred)
        for tree in self.trees:
            updates = np.array([self._predict_tree(tree, x) for x in X])
            F += self.learning_rate * updates
        # è½¬æ¢ä¸ºæ¦‚ç‡
        prob = 1 / (1 + np.exp(-F))
        return np.column_stack([1 - prob, prob])  # sklearn format

    def predict(self, X):
        """é¢„æµ‹ç±»åˆ«ï¼ˆ0/1ï¼‰"""
        return (self.predict_proba(X)[:, 1] > 0.5).astype(int)
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.metrics import roc_auc_score, classification_report

# Step 1: ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆè¿ç»­ç‰¹å¾ + purchase labelï¼‰
X, y = make_classification(
    n_samples=5000,
    n_features=10,
    n_informative=8,
    n_redundant=2,
    n_clusters_per_class=1,
    flip_y=0.01,
    class_sep=1.0,
    random_state=42
)

# ç‰¹å¾å…¨éƒ¨æ˜¯ floatï¼Œç¬¦åˆâ€œè¿ç»­ç‰¹å¾â€è¦æ±‚
print(f"X shape: {X.shape}, y mean: {y.mean():.3f}")

# Step 2: è®­ç»ƒè‡ªå®šä¹‰ GBDT æ¨¡å‹
gbdt = GBDTClassifier(
    n_estimators=50,
    max_depth=3,
    learning_rate=0.1,
    min_samples_split=10
)

gbdt.fit(X, y)

# Step 3: é¢„æµ‹
y_prob = gbdt.predict_proba(X)[:, 1]
y_pred = gbdt.predict(X)

# Step 4: è¯„ä¼°
auc = roc_auc_score(y, y_prob)
print(f"\nâœ… è‡ªå®šä¹‰ GBDT æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
print(f"AUC: {auc:.4f}")
print("\nğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y, y_pred, target_names=['æœªè´­ä¹°', 'è´­ä¹°']))
