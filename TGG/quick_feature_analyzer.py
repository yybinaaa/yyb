#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿç‰¹å¾ç»´åº¦åˆ†æå™¨

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬çš„ç‰¹å¾ç»´åº¦åˆ†æè„šæœ¬ï¼Œç”¨äºå¿«é€Ÿç”Ÿæˆç‰©å“å’Œç”¨æˆ·ç‰¹å¾çš„ç»´åº¦ä¿¡æ¯ã€‚
"""

import json
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, List, Any


def load_feature_data(data_dir: str) -> Dict[str, Any]:
    """
    åŠ è½½ç‰¹å¾æ•°æ®
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        åŒ…å«ç‰¹å¾æ•°æ®çš„å­—å…¸
    """
    data_dir = Path(data_dir)
    data = {}
    
    # åŠ è½½ç‰©å“ç‰¹å¾å­—å…¸
    item_feat_path = data_dir / "item_feat_dict.json"
    if item_feat_path.exists():
        with open(item_feat_path, 'r', encoding='utf-8') as f:
            data['item_feat_dict'] = json.load(f)
        print(f"âœ“ å·²åŠ è½½ç‰©å“ç‰¹å¾å­—å…¸ï¼ŒåŒ…å« {len(data['item_feat_dict'])} ä¸ªç‰©å“")
    else:
        print("âš  ç‰©å“ç‰¹å¾å­—å…¸æ–‡ä»¶ä¸å­˜åœ¨")
        data['item_feat_dict'] = {}
    
    # åŠ è½½ç´¢å¼•æ–‡ä»¶
    indexer_path = data_dir / "indexer.pkl"
    if indexer_path.exists():
        with open(indexer_path, 'rb') as f:
            data['indexer'] = pickle.load(f)
        print("âœ“ å·²åŠ è½½ç´¢å¼•æ–‡ä»¶")
    else:
        print("âš  ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
        data['indexer'] = {}
    
    return data


def define_feature_types() -> Dict[str, List[str]]:
    """
    å®šä¹‰ç‰¹å¾ç±»å‹
    
    Returns:
        ç‰¹å¾ç±»å‹å­—å…¸
    """
    return {
        'user_sparse': ['103', '104', '105', '109'],
        'item_sparse': ['100', '117', '111', '118', '101', '102', '119', '120', '114', '112', '121', '115', '122', '116'],
        'item_array': [],
        'user_array': ['106', '107', '108', '110'],
        'item_emb': ['81', '82', '83', '84', '85', '86'],  # å¤šæ¨¡æ€ç‰¹å¾ID
        'user_continual': [],
        'item_continual': []
    }


def analyze_feature_dimensions(data: Dict[str, Any], feature_types: Dict[str, List[str]]) -> Dict[str, Any]:
    """
    åˆ†æç‰¹å¾ç»´åº¦
    
    Args:
        data: ç‰¹å¾æ•°æ®
        feature_types: ç‰¹å¾ç±»å‹å®šä¹‰
        
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    print("æ­£åœ¨åˆ†æç‰¹å¾ç»´åº¦...")
    
    result = {
        'feature_counts': {},
        'sparse_feature_cardinalities': {},
        'embedding_dimensions': {},
        'array_feature_lengths': {},
        'feature_coverage': {}
    }
    
    # ç»Ÿè®¡å„ç±»å‹ç‰¹å¾æ•°é‡
    for feat_type, feat_ids in feature_types.items():
        result['feature_counts'][feat_type] = len(feat_ids)
    
    # åˆ†æç¨€ç–ç‰¹å¾åŸºæ•°
    if 'indexer' in data and 'f' in data['indexer']:
        for feat_type, feat_ids in feature_types.items():
            if 'sparse' in feat_type:
                for feat_id in feat_ids:
                    if feat_id in data['indexer']['f']:
                        cardinality = len(data['indexer']['f'][feat_id])
                        result['sparse_feature_cardinalities'][feat_id] = cardinality
    
    # å®šä¹‰Embeddingç‰¹å¾ç»´åº¦
    embedding_shapes = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
    for feat_id in feature_types['item_emb']:
        if feat_id in embedding_shapes:
            result['embedding_dimensions'][feat_id] = embedding_shapes[feat_id]
    
    # åˆ†ææ•°ç»„ç‰¹å¾é•¿åº¦
    if 'item_feat_dict' in data:
        for feat_type, feat_ids in feature_types.items():
            if 'array' in feat_type:
                for feat_id in feat_ids:
                    lengths = []
                    for item_id, item_feat in data['item_feat_dict'].items():
                        if feat_id in item_feat:
                            feat_value = item_feat[feat_id]
                            if isinstance(feat_value, list):
                                lengths.append(len(feat_value))
                    if lengths:
                        result['array_feature_lengths'][feat_id] = {
                            'min_length': min(lengths),
                            'max_length': max(lengths),
                            'avg_length': np.mean(lengths),
                            'std_length': np.std(lengths)
                        }
    
    # åˆ†æç‰¹å¾è¦†ç›–ç‡
    if 'item_feat_dict' in data:
        total_items = len(data['item_feat_dict'])
        for feat_type, feat_ids in feature_types.items():
            if feat_ids:
                coverage_stats = {}
                for feat_id in feat_ids:
                    covered_items = sum(1 for item_feat in data['item_feat_dict'].values() 
                                      if feat_id in item_feat)
                    coverage_rate = covered_items / total_items * 100
                    coverage_stats[feat_id] = coverage_rate
                result['feature_coverage'][feat_type] = coverage_stats
    
    print("âœ“ ç‰¹å¾ç»´åº¦åˆ†æå®Œæˆ")
    return result


def generate_dimension_summary(analysis_result: Dict[str, Any], feature_types: Dict[str, List[str]]) -> str:
    """
    ç”Ÿæˆç‰¹å¾ç»´åº¦æ‘˜è¦
    
    Args:
        analysis_result: åˆ†æç»“æœ
        feature_types: ç‰¹å¾ç±»å‹å®šä¹‰
        
    Returns:
        æ ¼å¼åŒ–çš„æ‘˜è¦å­—ç¬¦ä¸²
    """
    summary = []
    summary.append("=" * 60)
    summary.append("ç‰¹å¾ç»´åº¦æ‘˜è¦")
    summary.append("=" * 60)
    summary.append("")
    
    # ç‰¹å¾ç±»å‹ç»Ÿè®¡
    summary.append("1. ç‰¹å¾ç±»å‹ç»Ÿè®¡")
    summary.append("-" * 30)
    for feat_type, count in analysis_result['feature_counts'].items():
        summary.append(f"{feat_type:20}: {count:3d} ä¸ªç‰¹å¾")
    summary.append("")
    
    # ç¨€ç–ç‰¹å¾åŸºæ•°
    if analysis_result['sparse_feature_cardinalities']:
        summary.append("2. ç¨€ç–ç‰¹å¾åŸºæ•°")
        summary.append("-" * 30)
        for feat_id, cardinality in sorted(analysis_result['sparse_feature_cardinalities'].items()):
            category = "ç”¨æˆ·" if feat_id in feature_types['user_sparse'] else "ç‰©å“"
            summary.append(f"ç‰¹å¾ {feat_id:3s} ({category:2s}): {cardinality:8d} ä¸ªå”¯ä¸€å€¼")
        summary.append("")
    
    # Embeddingç‰¹å¾ç»´åº¦
    if analysis_result['embedding_dimensions']:
        summary.append("3. Embeddingç‰¹å¾ç»´åº¦")
        summary.append("-" * 30)
        for feat_id, dim in analysis_result['embedding_dimensions'].items():
            summary.append(f"ç‰¹å¾ {feat_id:3s}: {dim:4d} ç»´")
        summary.append("")
    
    # æ•°ç»„ç‰¹å¾é•¿åº¦
    if analysis_result['array_feature_lengths']:
        summary.append("4. æ•°ç»„ç‰¹å¾é•¿åº¦")
        summary.append("-" * 30)
        for feat_id, stats in analysis_result['array_feature_lengths'].items():
            category = "ç”¨æˆ·" if feat_id in feature_types['user_array'] else "ç‰©å“"
            summary.append(f"ç‰¹å¾ {feat_id:3s} ({category:2s}): é•¿åº¦èŒƒå›´ [{stats['min_length']:2d}, {stats['max_length']:2d}], "
                         f"å¹³å‡ {stats['avg_length']:5.2f}")
        summary.append("")
    
    # ç‰¹å¾è¦†ç›–ç‡
    if analysis_result['feature_coverage']:
        summary.append("5. ç‰¹å¾è¦†ç›–ç‡")
        summary.append("-" * 30)
        for feat_type, coverage_stats in analysis_result['feature_coverage'].items():
            if coverage_stats:
                coverage_str = " ".join([f"{feat_id}:{rate:5.1f}%" for feat_id, rate in coverage_stats.items()])
                summary.append(f"{feat_type:20}: {coverage_str}")
        summary.append("")
    
    summary.append("=" * 60)
    return "\n".join(summary)


def save_summary_to_file(summary: str, output_path: str = "feature_dimensions.txt"):
    """
    ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶
    
    Args:
        summary: æ‘˜è¦å†…å®¹
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(summary)
    print(f"âœ“ ç‰¹å¾ç»´åº¦æ‘˜è¦å·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python quick_feature_analyzer.py <æ•°æ®ç›®å½•è·¯å¾„> [è¾“å‡ºæ–‡ä»¶è·¯å¾„]")
        print("ç¤ºä¾‹: python quick_feature_analyzer.py ./data ./feature_dimensions.txt")
        sys.exit(1)
    
    data_dir = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else "feature_dimensions.txt"
    
    print(f"å¼€å§‹åˆ†ææ•°æ®ç›®å½•: {data_dir}")
    
    try:
        # 1. åŠ è½½æ•°æ®
        data = load_feature_data(data_dir)
        
        # 2. å®šä¹‰ç‰¹å¾ç±»å‹
        feature_types = define_feature_types()
        
        # 3. åˆ†æç‰¹å¾ç»´åº¦
        analysis_result = analyze_feature_dimensions(data, feature_types)
        
        # 4. ç”Ÿæˆæ‘˜è¦
        summary = generate_dimension_summary(analysis_result, feature_types)
        
        # 5. ä¿å­˜ç»“æœ
        save_summary_to_file(summary, output_path)
        
        # 6. æ‰“å°æ‘˜è¦
        print("\n" + summary)
        
        print(f"\nğŸ‰ ç‰¹å¾ç»´åº¦åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
