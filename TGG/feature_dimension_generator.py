#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰©å“å’Œç”¨æˆ·ç‰¹å¾ç»´åº¦ç”Ÿæˆå™¨

è¿™ä¸ªè„šæœ¬ç”¨äºåˆ†æå’Œç”Ÿæˆç‰©å“ã€ç”¨æˆ·ç‰¹å¾çš„ç»´åº¦ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
1. ç‰¹å¾ç±»å‹ç»Ÿè®¡
2. ç‰¹å¾ç»´åº¦åˆ†æ
3. ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡
4. ç”Ÿæˆç‰¹å¾ç»´åº¦æŠ¥å‘Š
"""

import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any, Tuple
import argparse


class FeatureDimensionGenerator:
    """
    ç‰¹å¾ç»´åº¦ç”Ÿæˆå™¨ç±»
    """
    
    def __init__(self, data_dir: str):
        """
        åˆå§‹åŒ–ç‰¹å¾ç»´åº¦ç”Ÿæˆå™¨
        
        Args:
            data_dir: æ•°æ®æ–‡ä»¶ç›®å½•è·¯å¾„
        """
        self.data_dir = Path(data_dir)
        self.item_feat_dict = None
        self.indexer = None
        self.mm_emb_dict = None
        self.feature_types = None
        self.feature_statistics = {}
        
    def load_data(self):
        """åŠ è½½å¿…è¦çš„æ•°æ®æ–‡ä»¶"""
        print("æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        # åŠ è½½ç‰©å“ç‰¹å¾å­—å…¸
        item_feat_path = self.data_dir / "item_feat_dict.json"
        if item_feat_path.exists():
            with open(item_feat_path, 'r', encoding='utf-8') as f:
                self.item_feat_dict = json.load(f)
            print(f"âœ“ å·²åŠ è½½ç‰©å“ç‰¹å¾å­—å…¸ï¼ŒåŒ…å« {len(self.item_feat_dict)} ä¸ªç‰©å“")
        else:
            print("âš  ç‰©å“ç‰¹å¾å­—å…¸æ–‡ä»¶ä¸å­˜åœ¨")
            
        # åŠ è½½ç´¢å¼•æ–‡ä»¶
        indexer_path = self.data_dir / "indexer.pkl"
        if indexer_path.exists():
            with open(indexer_path, 'rb') as f:
                self.indexer = pickle.load(f)
            print(f"âœ“ å·²åŠ è½½ç´¢å¼•æ–‡ä»¶")
        else:
            print("âš  ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
            
        # åŠ è½½å¤šæ¨¡æ€ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        mm_emb_path = self.data_dir / "creative_emb"
        if mm_emb_path.exists():
            self.mm_emb_dict = self._load_mm_emb(mm_emb_path)
            print(f"âœ“ å·²åŠ è½½å¤šæ¨¡æ€ç‰¹å¾")
        else:
            print("âš  å¤šæ¨¡æ€ç‰¹å¾ç›®å½•ä¸å­˜åœ¨")
            
        print("æ•°æ®åŠ è½½å®Œæˆï¼")
        
    def _load_mm_emb(self, mm_path: Path) -> Dict:
        """åŠ è½½å¤šæ¨¡æ€ç‰¹å¾Embedding"""
        SHAPE_DICT = {"81": 32, "82": 1024, "83": 3584, "84": 4096, "85": 3584, "86": 3584}
        mm_emb_dict = {}
        
        for feat_id, shape in SHAPE_DICT.items():
            try:
                if feat_id == '81':
                    emb_file = mm_path / f'emb_{feat_id}_{shape}.pkl'
                    if emb_file.exists():
                        with open(emb_file, 'rb') as f:
                            mm_emb_dict[feat_id] = pickle.load(f)
                else:
                    base_path = mm_path / f'emb_{feat_id}_{shape}'
                    if base_path.exists():
                        emb_dict = {}
                        for json_file in base_path.glob('*.json'):
                            with open(json_file, 'r', encoding='utf-8') as file:
                                for line in file:
                                    data_dict_origin = json.loads(line.strip())
                                    insert_emb = data_dict_origin['emb']
                                    if isinstance(insert_emb, list):
                                        insert_emb = np.array(insert_emb, dtype=np.float32)
                                    emb_dict[data_dict_origin['anonymous_cid']] = insert_emb
                        mm_emb_dict[feat_id] = emb_dict
            except Exception as e:
                print(f"åŠ è½½ç‰¹å¾ {feat_id} æ—¶å‡ºé”™: {e}")
                
        return mm_emb_dict
    
    def define_feature_types(self):
        """å®šä¹‰ç‰¹å¾ç±»å‹"""
        self.feature_types = {
            'user_sparse': ['103', '104', '105', '109'],
            'item_sparse': ['100', '117', '111', '118', '101', '102', '119', '120', '114', '112', '121', '115', '122', '116'],
            'item_array': [],
            'user_array': ['106', '107', '108', '110'],
            'item_emb': list(self.mm_emb_dict.keys()) if self.mm_emb_dict else [],
            'user_continual': [],
            'item_continual': []
        }
        print("âœ“ ç‰¹å¾ç±»å‹å®šä¹‰å®Œæˆ")
        
    def analyze_feature_dimensions(self) -> Dict[str, Any]:
        """åˆ†æç‰¹å¾ç»´åº¦"""
        print("æ­£åœ¨åˆ†æç‰¹å¾ç»´åº¦...")
        
        analysis_result = {
            'feature_counts': {},
            'feature_dimensions': {},
            'feature_distributions': {},
            'sparse_feature_cardinalities': {},
            'embedding_dimensions': {},
            'array_feature_lengths': {}
        }
        
        # åˆ†æç¨€ç–ç‰¹å¾åŸºæ•°
        if self.indexer and 'f' in self.indexer:
            for feat_type, feat_ids in self.feature_types.items():
                if 'sparse' in feat_type:
                    for feat_id in feat_ids:
                        if feat_id in self.indexer['f']:
                            cardinality = len(self.indexer['f'][feat_id])
                            analysis_result['sparse_feature_cardinalities'][feat_id] = cardinality
                            
        # åˆ†æEmbeddingç‰¹å¾ç»´åº¦
        if self.mm_emb_dict:
            for feat_id, emb_dict in self.mm_emb_dict.items():
                if emb_dict:
                    # è·å–ç¬¬ä¸€ä¸ªembeddingçš„ç»´åº¦
                    first_emb = next(iter(emb_dict.values()))
                    if hasattr(first_emb, 'shape'):
                        analysis_result['embedding_dimensions'][feat_id] = first_emb.shape[0]
                    elif isinstance(first_emb, list):
                        analysis_result['embedding_dimensions'][feat_id] = len(first_emb)
                        
        # åˆ†ææ•°ç»„ç‰¹å¾é•¿åº¦
        if self.item_feat_dict:
            for feat_type, feat_ids in self.feature_types.items():
                if 'array' in feat_type:
                    for feat_id in feat_ids:
                        lengths = []
                        for item_id, item_feat in self.item_feat_dict.items():
                            if feat_id in item_feat:
                                feat_value = item_feat[feat_id]
                                if isinstance(feat_value, list):
                                    lengths.append(len(feat_value))
                        if lengths:
                            analysis_result['array_feature_lengths'][feat_id] = {
                                'min_length': min(lengths),
                                'max_length': max(lengths),
                                'avg_length': np.mean(lengths),
                                'std_length': np.std(lengths)
                            }
                            
        # ç»Ÿè®¡å„ç±»å‹ç‰¹å¾æ•°é‡
        for feat_type, feat_ids in self.feature_types.items():
            analysis_result['feature_counts'][feat_type] = len(feat_ids)
            
        print("âœ“ ç‰¹å¾ç»´åº¦åˆ†æå®Œæˆ")
        return analysis_result
    
    def generate_feature_report(self, analysis_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆç‰¹å¾ç»´åº¦æŠ¥å‘Š"""
        print("æ­£åœ¨ç”Ÿæˆç‰¹å¾ç»´åº¦æŠ¥å‘Š...")
        
        report = []
        report.append("=" * 80)
        report.append("ç‰©å“å’Œç”¨æˆ·ç‰¹å¾ç»´åº¦åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)
        report.append("")
        
        # ç‰¹å¾ç±»å‹ç»Ÿè®¡
        report.append("1. ç‰¹å¾ç±»å‹ç»Ÿè®¡")
        report.append("-" * 40)
        for feat_type, count in analysis_result['feature_counts'].items():
            report.append(f"{feat_type:20}: {count:3d} ä¸ªç‰¹å¾")
        report.append("")
        
        # ç¨€ç–ç‰¹å¾åŸºæ•°
        if analysis_result['sparse_feature_cardinalities']:
            report.append("2. ç¨€ç–ç‰¹å¾åŸºæ•°ç»Ÿè®¡")
            report.append("-" * 40)
            for feat_id, cardinality in sorted(analysis_result['sparse_feature_cardinalities'].items()):
                report.append(f"ç‰¹å¾ {feat_id:3s}: {cardinality:8d} ä¸ªå”¯ä¸€å€¼")
            report.append("")
            
        # Embeddingç‰¹å¾ç»´åº¦
        if analysis_result['embedding_dimensions']:
            report.append("3. Embeddingç‰¹å¾ç»´åº¦")
            report.append("-" * 40)
            for feat_id, dim in analysis_result['embedding_dimensions'].items():
                report.append(f"ç‰¹å¾ {feat_id:3s}: {dim:4d} ç»´")
            report.append("")
            
        # æ•°ç»„ç‰¹å¾é•¿åº¦ç»Ÿè®¡
        if analysis_result['array_feature_lengths']:
            report.append("4. æ•°ç»„ç‰¹å¾é•¿åº¦ç»Ÿè®¡")
            report.append("-" * 40)
            for feat_id, stats in analysis_result['array_feature_lengths'].items():
                report.append(f"ç‰¹å¾ {feat_id:3s}: é•¿åº¦èŒƒå›´ [{stats['min_length']:2d}, {stats['max_length']:2d}], "
                           f"å¹³å‡ {stats['avg_length']:5.2f} Â± {stats['std_length']:5.2f}")
            report.append("")
            
        # ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡
        if self.item_feat_dict:
            report.append("5. ç‰¹å¾è¦†ç›–ç‡ç»Ÿè®¡")
            report.append("-" * 40)
            total_items = len(self.item_feat_dict)
            for feat_type, feat_ids in self.feature_types.items():
                if feat_ids:
                    coverage_stats = []
                    for feat_id in feat_ids:
                        covered_items = sum(1 for item_feat in self.item_feat_dict.values() 
                                          if feat_id in item_feat)
                        coverage_rate = covered_items / total_items * 100
                        coverage_stats.append(f"{feat_id}:{coverage_rate:5.1f}%")
                    report.append(f"{feat_type:20}: {' '.join(coverage_stats)}")
            report.append("")
            
        report.append("=" * 80)
        report.append("æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    def save_feature_report(self, report: str, output_path: str = "feature_dimension_report.txt"):
        """ä¿å­˜ç‰¹å¾ç»´åº¦æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        output_file = Path(output_path)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"âœ“ ç‰¹å¾ç»´åº¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
    def generate_feature_summary_csv(self, analysis_result: Dict[str, Any], output_path: str = "feature_summary.csv"):
        """ç”Ÿæˆç‰¹å¾æ‘˜è¦CSVæ–‡ä»¶"""
        print("æ­£åœ¨ç”Ÿæˆç‰¹å¾æ‘˜è¦CSVæ–‡ä»¶...")
        
        summary_data = []
        
        # ç¨€ç–ç‰¹å¾
        for feat_id, cardinality in analysis_result['sparse_feature_cardinalities'].items():
            summary_data.append({
                'feature_id': feat_id,
                'feature_type': 'sparse',
                'category': 'user' if feat_id in self.feature_types['user_sparse'] else 'item',
                'dimension': cardinality,
                'description': f'ç¨€ç–ç‰¹å¾ï¼ŒåŸºæ•°: {cardinality}'
            })
            
        # Embeddingç‰¹å¾
        for feat_id, dim in analysis_result['embedding_dimensions'].items():
            summary_data.append({
                'feature_id': feat_id,
                'feature_type': 'embedding',
                'category': 'item',
                'dimension': dim,
                'description': f'å¤šæ¨¡æ€Embeddingï¼Œç»´åº¦: {dim}'
            })
            
        # æ•°ç»„ç‰¹å¾
        for feat_id, stats in analysis_result['array_feature_lengths'].items():
            summary_data.append({
                'feature_id': feat_id,
                'feature_type': 'array',
                'category': 'user' if feat_id in self.feature_types['user_array'] else 'item',
                'dimension': int(stats['avg_length']),
                'description': f'æ•°ç»„ç‰¹å¾ï¼Œå¹³å‡é•¿åº¦: {stats["avg_length"]:.1f}'
            })
            
        # è¿ç»­ç‰¹å¾
        for feat_type in ['user_continual', 'item_continual']:
            for feat_id in self.feature_types[feat_type]:
                summary_data.append({
                    'feature_id': feat_id,
                    'feature_type': 'continual',
                    'category': 'user' if feat_type == 'user_continual' else 'item',
                    'dimension': 1,
                    'description': 'è¿ç»­æ•°å€¼ç‰¹å¾'
                })
                
        # è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(summary_data)
        df = df.sort_values(['category', 'feature_type', 'feature_id'])
        
        output_file = Path(output_path)
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"âœ“ ç‰¹å¾æ‘˜è¦CSVæ–‡ä»¶å·²ä¿å­˜åˆ°: {output_file}")
        
        return df
    
    def visualize_feature_dimensions(self, analysis_result: Dict[str, Any], output_dir: str = "feature_visualizations"):
        """ç”Ÿæˆç‰¹å¾ç»´åº¦å¯è§†åŒ–å›¾è¡¨"""
        print("æ­£åœ¨ç”Ÿæˆç‰¹å¾ç»´åº¦å¯è§†åŒ–å›¾è¡¨...")
        
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. ç‰¹å¾ç±»å‹åˆ†å¸ƒé¥¼å›¾
        plt.figure(figsize=(10, 8))
        feat_counts = analysis_result['feature_counts']
        plt.pie(feat_counts.values(), labels=feat_counts.keys(), autopct='%1.1f%%', startangle=90)
        plt.title('ç‰¹å¾ç±»å‹åˆ†å¸ƒ')
        plt.savefig(output_dir / 'feature_type_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ç¨€ç–ç‰¹å¾åŸºæ•°æŸ±çŠ¶å›¾
        if analysis_result['sparse_feature_cardinalities']:
            plt.figure(figsize=(12, 6))
            feat_ids = list(analysis_result['sparse_feature_cardinalities'].keys())
            cardinalities = list(analysis_result['sparse_feature_cardinalities'].values())
            
            colors = ['skyblue' if feat_id in self.feature_types['user_sparse'] else 'lightcoral' 
                     for feat_id in feat_ids]
            
            plt.bar(feat_ids, cardinalities, color=colors, alpha=0.7)
            plt.title('ç¨€ç–ç‰¹å¾åŸºæ•°åˆ†å¸ƒ')
            plt.xlabel('ç‰¹å¾ID')
            plt.ylabel('åŸºæ•°')
            plt.xticks(rotation=45)
            plt.legend(['ç”¨æˆ·ç‰¹å¾', 'ç‰©å“ç‰¹å¾'])
            plt.tight_layout()
            plt.savefig(output_dir / 'sparse_feature_cardinalities.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        # 3. Embeddingç‰¹å¾ç»´åº¦å›¾
        if analysis_result['embedding_dimensions']:
            plt.figure(figsize=(10, 6))
            feat_ids = list(analysis_result['embedding_dimensions'].keys())
            dimensions = list(analysis_result['embedding_dimensions'].values())
            
            plt.bar(feat_ids, dimensions, color='lightgreen', alpha=0.7)
            plt.title('Embeddingç‰¹å¾ç»´åº¦')
            plt.xlabel('ç‰¹å¾ID')
            plt.ylabel('ç»´åº¦')
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(output_dir / 'embedding_feature_dimensions.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        print(f"âœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
        
    def run_full_analysis(self, output_dir: str = "feature_analysis_output"):
        """è¿è¡Œå®Œæ•´çš„ç‰¹å¾åˆ†ææµç¨‹"""
        print("å¼€å§‹è¿è¡Œå®Œæ•´çš„ç‰¹å¾åˆ†ææµç¨‹...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # 1. åŠ è½½æ•°æ®
        self.load_data()
        
        # 2. å®šä¹‰ç‰¹å¾ç±»å‹
        self.define_feature_types()
        
        # 3. åˆ†æç‰¹å¾ç»´åº¦
        analysis_result = self.analyze_feature_dimensions()
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report = self.generate_feature_report(analysis_result)
        
        # 5. ä¿å­˜æŠ¥å‘Š
        self.save_feature_report(report, output_dir / "feature_dimension_report.txt")
        
        # 6. ç”ŸæˆCSVæ‘˜è¦
        summary_df = self.generate_feature_summary_csv(analysis_result, output_dir / "feature_summary.csv")
        
        # 7. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.visualize_feature_dimensions(analysis_result, output_dir / "visualizations")
        
        print(f"\nğŸ‰ ç‰¹å¾åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        print(f"ğŸ“Š æŠ¥å‘Šæ–‡ä»¶: {output_dir / 'feature_dimension_report.txt'}")
        print(f"ğŸ“‹ æ‘˜è¦æ–‡ä»¶: {output_dir / 'feature_summary.csv'}")
        print(f"ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨: {output_dir / 'visualizations'}")
        
        return analysis_result, summary_df


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç‰©å“å’Œç”¨æˆ·ç‰¹å¾ç»´åº¦ç”Ÿæˆå™¨')
    parser.add_argument('--data_dir', type=str, required=True, help='æ•°æ®æ–‡ä»¶ç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='feature_analysis_output', help='è¾“å‡ºç›®å½•è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç‰¹å¾ç»´åº¦ç”Ÿæˆå™¨
    generator = FeatureDimensionGenerator(args.data_dir)
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    try:
        analysis_result, summary_df = generator.run_full_analysis(args.output_dir)
        print("\nåˆ†æç»“æœé¢„è§ˆ:")
        print(summary_df.head(10))
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
